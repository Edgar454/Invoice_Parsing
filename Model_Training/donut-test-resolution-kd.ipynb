{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1400106,"sourceType":"datasetVersion","datasetId":818027}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q jiwer mlflow datasets peft prettytable evaluate optuna --upgrade ","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:04.691142Z","iopub.execute_input":"2024-06-28T20:16:04.691522Z","iopub.status.idle":"2024-06-28T20:16:09.972038Z","shell.execute_reply.started":"2024-06-28T20:16:04.691488Z","shell.execute_reply":"2024-06-28T20:16:09.970957Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\n\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install  -q transformers==4.40.0","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:09.974276Z","iopub.execute_input":"2024-06-28T20:16:09.974720Z","iopub.status.idle":"2024-06-28T20:16:33.739002Z","shell.execute_reply.started":"2024-06-28T20:16:09.974683Z","shell.execute_reply":"2024-06-28T20:16:33.737979Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:33.740324Z","iopub.execute_input":"2024-06-28T20:16:33.740609Z","iopub.status.idle":"2024-06-28T20:16:38.848047Z","shell.execute_reply.started":"2024-06-28T20:16:33.740582Z","shell.execute_reply":"2024-06-28T20:16:38.847115Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"4.40.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader , Dataset\nfrom torchvision.utils import make_grid\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport os\nfrom pathlib import Path\nfrom glob import glob\nfrom tqdm.auto import tqdm\nimport tempfile\n\nimport random\nimport time\nimport cv2\nimport sys\nfrom prettytable import PrettyTable\n\nimport re\nimport string\nimport shutil\n\nimport optuna \nfrom datasets import load_dataset , concatenate_datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:38.850747Z","iopub.execute_input":"2024-06-28T20:16:38.851306Z","iopub.status.idle":"2024-06-28T20:16:41.375885Z","shell.execute_reply.started":"2024-06-28T20:16:38.851278Z","shell.execute_reply":"2024-06-28T20:16:41.375128Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Handwritten Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_1 = load_dataset('shivi/cheques_sample_data' )","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:41.377018Z","iopub.execute_input":"2024-06-28T20:16:41.377585Z","iopub.status.idle":"2024-06-28T20:16:46.380811Z","shell.execute_reply.started":"2024-06-28T20:16:41.377559Z","shell.execute_reply":"2024-06-28T20:16:46.379981Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/540 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a965811cec3a46108526214b78795925"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/41.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7176e51055149aca98fdc96fbc7f74e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a83a6be28c412d983656f33e544e4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.87M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b844c9785f5049a3b2c3add3b908c85f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18c656370dc4612971c7761d0d9b7b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b304f40af37403dbe03d456beac99e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a4f87b64ca4225a2895c3fc9be19b2"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_dataset(sample):\n  text =  json.loads(sample['ground_truth'])\n  details = text['gt_parse']['cheque_details']\n  target = details[0]\n  for detail in details :\n    target.update(detail)\n\n  return {'image':sample['image'] ,'ground_truth': str(target) }","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:46.381887Z","iopub.execute_input":"2024-06-28T20:16:46.382209Z","iopub.status.idle":"2024-06-28T20:16:46.387655Z","shell.execute_reply.started":"2024-06-28T20:16:46.382184Z","shell.execute_reply":"2024-06-28T20:16:46.386745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"processed_dataset = dataset_1.map(preprocess_dataset , remove_columns = ['ground_truth'])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:46.389035Z","iopub.execute_input":"2024-06-28T20:16:46.389321Z","iopub.status.idle":"2024-06-28T20:16:52.532992Z","shell.execute_reply.started":"2024-06-28T20:16:46.389298Z","shell.execute_reply":"2024-06-28T20:16:52.532001Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e40fbf373dd44a6afa3a3dd66a77bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a2c228f5bc4dad858bdf9d810fdb86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ede1919940694b4591eb1ab960dd453f"}},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset['test']= processed_dataset['test'].select(range(20))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:52.534170Z","iopub.execute_input":"2024-06-28T20:16:52.534453Z","iopub.status.idle":"2024-06-28T20:16:52.541763Z","shell.execute_reply.started":"2024-06-28T20:16:52.534427Z","shell.execute_reply":"2024-06-28T20:16:52.540694Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from ast import literal_eval","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:52.542984Z","iopub.execute_input":"2024-06-28T20:16:52.543262Z","iopub.status.idle":"2024-06-28T20:16:52.575394Z","shell.execute_reply.started":"2024-06-28T20:16:52.543239Z","shell.execute_reply":"2024-06-28T20:16:52.574482Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderConfig\n\nimage_size = [1280,1920]\nmax_length = 512\n\n# update image_size of the encoder\n# during pre-training, a larger image size was used\nconfig = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\nconfig.encoder.image_size = image_size # (height, width)\n# update max_length of the decoder (for generation)\nconfig.decoder.max_length = max_length","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:52.578768Z","iopub.execute_input":"2024-06-28T20:16:52.579534Z","iopub.status.idle":"2024-06-28T20:16:52.862528Z","shell.execute_reply.started":"2024-06-28T20:16:52.579503Z","shell.execute_reply":"2024-06-28T20:16:52.861662Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d100c78a02414c8aa7d0e2a162a891d9"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DonutProcessor, VisionEncoderDecoderModel\n\nprocessor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:16:52.863667Z","iopub.execute_input":"2024-06-28T20:16:52.863975Z","iopub.status.idle":"2024-06-28T20:17:13.990914Z","shell.execute_reply.started":"2024-06-28T20:16:52.863915Z","shell.execute_reply":"2024-06-28T20:17:13.989632Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-06-28 20:16:55.161160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-28 20:16:55.161278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-28 20:16:55.317298: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/362 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a0488033f9412d9a6d7d606ecdd402"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/518 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfed2d8eb7e14412b1755fc1baa7cf8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf9e47b6fbc445ff91744f6566d3fb2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2481a19a90d644afa6a2e6a0b08ac6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/71.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605edcf3236e45fbbca93aa2d13fad03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb172be4d9e1421b8ed81b5930ee3b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc9a9570dd64bcdbb80d80d1207ecfa"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Any, List, Tuple\nadded_tokens = []\n\n\nclass DonutDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for Donut. This class takes a HuggingFace Dataset as input.\n\n    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n    and it will be converted into pixel_values (vectorized image) and labels (input_ids of the tokenized string).\n\n    Args:\n        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n        max_length: the max number of tokens for the target sequences\n        split: whether to load \"train\", \"validation\" or \"test\" split\n        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n        task_start_token: the special token to be fed to the decoder to conduct the target task\n        prompt_end_token: the special token at the end of the sequences\n        sort_json_key: whether or not to sort the JSON keys\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        max_length: int,\n        split: str = \"train\",\n        ignore_id: int = -100,\n        task_start_token: str = \"<s>\",\n        prompt_end_token: str = None,\n        sort_json_key: bool = True,\n    ):\n        super().__init__()\n\n        self.max_length = max_length\n        self.split = split\n        self.ignore_id = ignore_id\n        self.task_start_token = task_start_token\n        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n        self.sort_json_key = sort_json_key\n\n        self.dataset = dataset[self.split]\n        self.dataset_length = len(self.dataset)\n\n        self.gt_token_sequences = []\n        for sample in self.dataset:\n            ground_truth = literal_eval(sample[\"ground_truth\"])\n            assert isinstance(ground_truth, dict)\n            gt_jsons = [ground_truth]\n\n            self.gt_token_sequences.append(\n                [\n                    self.json2token(\n                        gt_jsons,\n                        update_special_tokens_for_json_key=self.split == \"train\",\n                        sort_json_key=self.sort_json_key,\n                    )\n                    + processor.tokenizer.eos_token\n\n                ]\n            )\n\n        self.add_tokens([self.task_start_token, self.prompt_end_token])\n        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n\n    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n        \"\"\"\n        Convert an ordered JSON object into a token sequence\n        \"\"\"\n        if type(obj) == dict:\n                output = \"\"\n                if sort_json_key:\n                    keys = sorted(obj.keys(), reverse=True)\n                else:\n                    keys = obj.keys()\n\n                for k in keys:\n                    if update_special_tokens_for_json_key:\n                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n                    output += (\n                        fr\"<s_{k}>\"\n                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n                        + fr\"</s_{k}>\"\n                    )\n                return output\n\n        elif type(obj) == list:\n            return r\"<sep/>\".join(\n                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n            )\n        else:\n            obj = str(obj)\n            if f\"<{obj}/>\" in added_tokens:\n                obj = f\"<{obj}/>\"  # for categorical special tokens\n            return obj\n\n    def add_tokens(self, list_of_tokens: List[str]):\n        \"\"\"\n        Add special tokens to tokenizer and resize the token embeddings of the decoder\n        \"\"\"\n        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n        if newly_added_num > 0:\n            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n            added_tokens.extend(list_of_tokens)\n\n    def __len__(self) -> int:\n        return self.dataset_length\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Load image from image_path of given dataset_path and convert into input_tensor and labels\n        Convert gt data into input_ids (tokenized string)\n        Returns:\n            input_tensor : preprocessed image\n            input_ids : tokenized gt_data\n            labels : masked labels (model doesn't need to predict prompt and pad token)\n        \"\"\"\n        sample = self.dataset[idx]\n\n        # inputs\n        pixel_values = processor(sample[\"image\"].convert('RGB'), random_padding= self.split == \"train\", return_tensors=\"pt\").pixel_values\n        pixel_values = pixel_values.squeeze()\n\n        # targets\n        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n        input_ids = processor.tokenizer(\n            target_sequence,\n            add_special_tokens=False,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )[\"input_ids\"].squeeze(0)\n\n        labels = input_ids.clone()\n        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n        return pixel_values, labels, target_sequence","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:13.992899Z","iopub.execute_input":"2024-06-28T20:17:13.993206Z","iopub.status.idle":"2024-06-28T20:17:14.014110Z","shell.execute_reply.started":"2024-06-28T20:17:13.993180Z","shell.execute_reply":"2024-06-28T20:17:14.013181Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"processor.image_processor.size = image_size[::-1] # should be (width, height)\nprocessor.image_processor.do_align_long_axis = False\n\ntrain_dataset = DonutDataset(processed_dataset, max_length=max_length,\n                             split=\"train\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n                             sort_json_key=True, # cord dataset is preprocessed, so no need for this\n                             )\n\nval_dataset = DonutDataset(processed_dataset, max_length=max_length,\n                             split=\"test\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n                             sort_json_key=True, # cord dataset is preprocessed, so no need for this\n                             )","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:14.015279Z","iopub.execute_input":"2024-06-28T20:17:14.015581Z","iopub.status.idle":"2024-06-28T20:17:25.329851Z","shell.execute_reply.started":"2024-06-28T20:17:14.015548Z","shell.execute_reply":"2024-06-28T20:17:25.329079Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(added_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.331000Z","iopub.execute_input":"2024-06-28T20:17:25.331284Z","iopub.status.idle":"2024-06-28T20:17:25.339246Z","shell.execute_reply.started":"2024-06-28T20:17:25.331260Z","shell.execute_reply":"2024-06-28T20:17:25.338370Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"print(added_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.340269Z","iopub.execute_input":"2024-06-28T20:17:25.340532Z","iopub.status.idle":"2024-06-28T20:17:25.355140Z","shell.execute_reply.started":"2024-06-28T20:17:25.340509Z","shell.execute_reply":"2024-06-28T20:17:25.354233Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"['<s_payee_name>', '</s_payee_name>', '<s_cheque_date>', '</s_cheque_date>', '<s_bank_name>', '</s_bank_name>', '<s_amt_in_words>', '</s_amt_in_words>', '<s_amt_in_figures>', '</s_amt_in_figures>', '<s_cord-v2>', '<s_cord-v2>']\n","output_type":"stream"}]},{"cell_type":"code","source":"# the vocab size attribute stays constants (might be a bit unintuitive - but doesn't include special tokens)\nprint(\"Original number of tokens:\", processor.tokenizer.vocab_size)\nprint(\"Number of tokens after adding special tokens:\", len(processor.tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.356162Z","iopub.execute_input":"2024-06-28T20:17:25.356432Z","iopub.status.idle":"2024-06-28T20:17:25.375870Z","shell.execute_reply.started":"2024-06-28T20:17:25.356400Z","shell.execute_reply":"2024-06-28T20:17:25.374997Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Original number of tokens: 57522\nNumber of tokens after adding special tokens: 57536\n","output_type":"stream"}]},{"cell_type":"code","source":"processor.decode([57527])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.376954Z","iopub.execute_input":"2024-06-28T20:17:25.377248Z","iopub.status.idle":"2024-06-28T20:17:25.387784Z","shell.execute_reply.started":"2024-06-28T20:17:25.377225Z","shell.execute_reply":"2024-06-28T20:17:25.386988Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'<s_cheque_date>'"},"metadata":{}}]},{"cell_type":"code","source":"pixel_values, labels, target_sequence = train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.388733Z","iopub.execute_input":"2024-06-28T20:17:25.388989Z","iopub.status.idle":"2024-06-28T20:17:25.641369Z","shell.execute_reply.started":"2024-06-28T20:17:25.388966Z","shell.execute_reply":"2024-06-28T20:17:25.640600Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(pixel_values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.642538Z","iopub.execute_input":"2024-06-28T20:17:25.642974Z","iopub.status.idle":"2024-06-28T20:17:25.647942Z","shell.execute_reply.started":"2024-06-28T20:17:25.642924Z","shell.execute_reply":"2024-06-28T20:17:25.646952Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"torch.Size([3, 1280, 1920])\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's print the labels (the first 30 token ID's)\nfor id in labels.tolist()[:30]:\n  if id != -100:\n    print(processor.decode([id]))\n  else:\n    print(id)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.649046Z","iopub.execute_input":"2024-06-28T20:17:25.649401Z","iopub.status.idle":"2024-06-28T20:17:25.658492Z","shell.execute_reply.started":"2024-06-28T20:17:25.649369Z","shell.execute_reply":"2024-06-28T20:17:25.657636Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"<s_payee_name>\nEd\nmee\nP\nelle\ntier\n</s_payee_name>\n<s_cheque_date>\n06\n/\n05\n/\n22\n</s_cheque_date>\n<s_bank_name>\nA\nXI\nS\nBA\nNK\n</s_bank_name>\n<s_amt_in_words>\nTh\nre\ne\nTho\nus\nand\nSe\nven\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's check the corresponding target sequence, as a string\nprint(target_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.659560Z","iopub.execute_input":"2024-06-28T20:17:25.659829Z","iopub.status.idle":"2024-06-28T20:17:25.667960Z","shell.execute_reply.started":"2024-06-28T20:17:25.659800Z","shell.execute_reply":"2024-06-28T20:17:25.667054Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"<s_payee_name>Edmee Pelletier</s_payee_name><s_cheque_date>06/05/22</s_cheque_date><s_bank_name>AXIS BANK</s_bank_name><s_amt_in_words>Three Thousand Seven Hundred and Fifty Five</s_amt_in_words><s_amt_in_figures>3755</s_amt_in_figures></s>\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_cord-v2>'])[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.669067Z","iopub.execute_input":"2024-06-28T20:17:25.669642Z","iopub.status.idle":"2024-06-28T20:17:25.677793Z","shell.execute_reply.started":"2024-06-28T20:17:25.669604Z","shell.execute_reply":"2024-06-28T20:17:25.676980Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# sanity check\nprint(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\nprint(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.678854Z","iopub.execute_input":"2024-06-28T20:17:25.679190Z","iopub.status.idle":"2024-06-28T20:17:25.687775Z","shell.execute_reply.started":"2024-06-28T20:17:25.679158Z","shell.execute_reply":"2024-06-28T20:17:25.686974Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Pad token ID: <pad>\nDecoder start token ID: <s_cord-v2>\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token = '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')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.689019Z","iopub.execute_input":"2024-06-28T20:17:25.689272Z","iopub.status.idle":"2024-06-28T20:17:25.777814Z","shell.execute_reply.started":"2024-06-28T20:17:25.689251Z","shell.execute_reply":"2024-06-28T20:17:25.776871Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating pytorch dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# feel free to increase the batch size if you have a lot of memory\n# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.778957Z","iopub.execute_input":"2024-06-28T20:17:25.779243Z","iopub.status.idle":"2024-06-28T20:17:25.784363Z","shell.execute_reply.started":"2024-06-28T20:17:25.779219Z","shell.execute_reply":"2024-06-28T20:17:25.783433Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\npixel_values, labels, target_sequences = batch\nprint(pixel_values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:25.785605Z","iopub.execute_input":"2024-06-28T20:17:25.785869Z","iopub.status.idle":"2024-06-28T20:17:26.840557Z","shell.execute_reply.started":"2024-06-28T20:17:25.785847Z","shell.execute_reply":"2024-06-28T20:17:26.839450Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 1280, 1920])\n","output_type":"stream"}]},{"cell_type":"code","source":"for id in labels[0].squeeze().tolist()[:30]:\n  if id != -100:\n    print(processor.decode([id]))\n  else:\n    print(id)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:26.842166Z","iopub.execute_input":"2024-06-28T20:17:26.842546Z","iopub.status.idle":"2024-06-28T20:17:26.850523Z","shell.execute_reply.started":"2024-06-28T20:17:26.842506Z","shell.execute_reply":"2024-06-28T20:17:26.849644Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"<s_payee_name>\nEt\nhel\nM\n.\nBry\nson\n</s_payee_name>\n<s_cheque_date>\n06\n/\n05\n/\n22\n</s_cheque_date>\n<s_bank_name>\nCan\nara\nBank\n</s_bank_name>\n<s_amt_in_words>\nSe\nven\nHund\nred\nand\nE\night\ny\nSix\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(val_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:26.855881Z","iopub.execute_input":"2024-06-28T20:17:26.856605Z","iopub.status.idle":"2024-06-28T20:17:26.862250Z","shell.execute_reply.started":"2024-06-28T20:17:26.856561Z","shell.execute_reply":"2024-06-28T20:17:26.861385Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"2800\n20\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's check the first validation batch\nbatch = next(iter(val_dataloader))\npixel_values, labels, target_sequences = batch\nprint(pixel_values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:26.863476Z","iopub.execute_input":"2024-06-28T20:17:26.864178Z","iopub.status.idle":"2024-06-28T20:17:27.915875Z","shell.execute_reply.started":"2024-06-28T20:17:26.864144Z","shell.execute_reply":"2024-06-28T20:17:27.914785Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 1280, 1920])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(target_sequences[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:27.917464Z","iopub.execute_input":"2024-06-28T20:17:27.917774Z","iopub.status.idle":"2024-06-28T20:17:27.923032Z","shell.execute_reply.started":"2024-06-28T20:17:27.917744Z","shell.execute_reply":"2024-06-28T20:17:27.921974Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"<s_payee_name>Seymour Patenaude</s_payee_name><s_cheque_date>06/05/22</s_cheque_date><s_bank_name>Canara Bank</s_bank_name><s_amt_in_words>Two Thousand Thre Hundred and Twenty Two</s_amt_in_words><s_amt_in_figures>2322</s_amt_in_figures></s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define LightningModule","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport re\nfrom nltk import edit_distance\nimport numpy as np\nimport math\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import LambdaLR , ReduceLROnPlateau\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass DonutModelPLModule(pl.LightningModule):\n    def __init__(self, config, processor, model):\n        super().__init__()\n        self.config = config\n        self.processor = processor\n        self.model = model\n\n    def training_step(self, batch, batch_idx):\n        pixel_values, labels, _ = batch\n        \n        outputs = self.model(pixel_values, labels=labels)\n        loss = outputs.loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx, dataset_idx=0):\n        pixel_values, labels, answers = batch\n        batch_size = pixel_values.shape[0]\n        # we feed the prompt to the model\n        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n        \n        outputs = self.model.generate(pixel_values,\n                                   decoder_input_ids=decoder_input_ids,\n                                   max_length=max_length,\n                                   early_stopping=True,\n                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n                                   use_cache=True,\n                                   num_beams=1,\n                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n                                   return_dict_in_generate=True,)\n    \n        predictions = []\n        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n            predictions.append(seq)\n\n        scores = []\n        for pred, answer in zip(predictions, answers):\n            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n            # NOT NEEDED ANYMORE\n            # answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n\n            if self.config.get(\"verbose\", False) and len(scores) == 1:\n                print(f\"Prediction: {pred}\")\n                print(f\"    Answer: {answer}\")\n                print(f\" Normed ED: {scores[0]}\")\n\n        self.log(\"val_edit_distance\", np.mean(scores))\n        \n        return scores\n\n    def configure_optimizers(self):\n        # you could also add a learning rate scheduler if you want\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n    \n        return [optimizer],[{\"scheduler\":scheduler,\n                            \"monitor\": \"val_edit_distance\" }]\n\n    def train_dataloader(self):\n        return train_dataloader\n\n    def val_dataloader(self):\n        return val_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:27.924262Z","iopub.execute_input":"2024-06-28T20:17:27.924537Z","iopub.status.idle":"2024-06-28T20:17:31.248895Z","shell.execute_reply.started":"2024-06-28T20:17:27.924514Z","shell.execute_reply":"2024-06-28T20:17:31.247905Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"config = {\"max_epochs\":2,\n          \"val_check_interval\":0.2, # how many times we want to validate during an epoch\n          \"check_val_every_n_epoch\":1,\n          \"gradient_clip_val\":1.0,\n          \"num_training_samples_per_epoch\": 800,\n          \"lr\":3e-5,\n          \"train_batch_sizes\": [8],\n          \"val_batch_sizes\": [1],\n          # \"seed\":2022,\n          \"num_nodes\": 1,\n          \"warmup_steps\": 300, # 800/8*30/10, 10%\n          \"result_path\": \"./result\",\n          \"verbose\": False,\n          }\n\nmodel_module = DonutModelPLModule(config, processor, model)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:31.250284Z","iopub.execute_input":"2024-06-28T20:17:31.251009Z","iopub.status.idle":"2024-06-28T20:17:31.256945Z","shell.execute_reply.started":"2024-06-28T20:17:31.250973Z","shell.execute_reply":"2024-06-28T20:17:31.255963Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# import wandb\n# wandb.init(project=\"Donut_Knowledge_Distillation\", name=\"demo-run-cord-500x500\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:31.258097Z","iopub.execute_input":"2024-06-28T20:17:31.258414Z","iopub.status.idle":"2024-06-28T20:17:31.269218Z","shell.execute_reply.started":"2024-06-28T20:17:31.258384Z","shell.execute_reply":"2024-06-28T20:17:31.268375Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping , ModelCheckpoint ,ModelPruning\n\nwandb_logger = WandbLogger(project=\"Donut_Knowledge_Distillation\", name=\"demo-run-cord-1280x1920\")\n\nclass PushToHubCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n        pl_module.model.push_to_hub(\"Edgar404/donut-shivi-cheques_1920\",\n                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n\n    def on_train_end(self, trainer, pl_module):\n        print(f\"Pushing model to the hub after training\")\n        pl_module.processor.push_to_hub(\"Edgar404/donut-shivi-cheques_1920\",\n                                    commit_message=f\"Training done\")\n        pl_module.model.push_to_hub(\"Edgar404/donut-shivi-cheques_1920\",\n                                    commit_message=f\"Training done\")\n\nearly_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=10, verbose=False, mode=\"min\")\ncheckpoint = ModelCheckpoint(monitor =\"val_edit_distance\", dirpath = './best_model')\n# pruner = ModelPruning(\"ln_structured\",pruning_norm=1,pruning_dim=0 ,amount= 0.5 ,use_global_unstructured= False)\n\ntrainer = pl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        max_epochs=config.get(\"max_epochs\"),\n        val_check_interval=config.get(\"val_check_interval\"),\n        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n        gradient_clip_val=config.get(\"gradient_clip_val\"),\n        precision=32, # we'll use mixed precision\n        num_sanity_val_steps=0,\n        logger=wandb_logger,\n        callbacks=[PushToHubCallback(), early_stop_callback , checkpoint ],)\n\ntrainer.fit(model_module)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T20:17:31.270347Z","iopub.execute_input":"2024-06-28T20:17:31.270912Z","iopub.status.idle":"2024-06-28T22:06:43.768886Z","shell.execute_reply.started":"2024-06-28T20:17:31.270880Z","shell.execute_reply":"2024-06-28T22:06:43.767851Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20240628_202156-rary1es5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation/runs/rary1es5' target=\"_blank\">demo-run-cord-1280x1920</a></strong> to <a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation' target=\"_blank\">https://wandb.ai/eded_404/Donut_Knowledge_Distillation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation/runs/rary1es5' target=\"_blank\">https://wandb.ai/eded_404/Donut_Knowledge_Distillation/runs/rary1es5</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working/best_model exists and is not empty.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33cdd40e2d7c42fc886432eb9d68922f"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub, epoch 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f994f3f23eee41bc952124091f1b1bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbf0bef2fdc40928eea5ef01bd665c1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub, epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"226eb43f356c44e3bc7db803f8bdd28f"}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub after training\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model.push_to_hub(\"Edgar404/donut-shivi-cheques_1920\",\n                                    commit_message=f\"Best model updated\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:00:19.554269Z","iopub.execute_input":"2024-06-05T12:00:19.555007Z","iopub.status.idle":"2024-06-05T12:00:45.255716Z","shell.execute_reply.started":"2024-06-05T12:00:19.554975Z","shell.execute_reply":"2024-06-05T12:00:45.254705Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279a578dff2d4e70ae7cf8698b3acdf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbabc5ccbcfc4491b994cf01406e8771"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Edgar404/donut-shivi-cheques_1920/commit/bf4f740d7f1e8a59ff042daab5fce0bfae115489', commit_message='Best model updated', commit_description='', oid='bf4f740d7f1e8a59ff042daab5fce0bfae115489', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"#teacher import\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel\n\nteacher_model = VisionEncoderDecoderModel.from_pretrained(\"Edgar404/donut-shivi-cheques-cheques_1920\")\nteacher_processor = DonutProcessor.from_pretrained(\"Edgar404/donut-shivi-cheques-cheques_1920\")\nteacher_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:25:17.217973Z","iopub.execute_input":"2024-06-28T13:25:17.218948Z","iopub.status.idle":"2024-06-28T13:25:19.319382Z","shell.execute_reply.started":"2024-06-28T13:25:17.218916Z","shell.execute_reply":"2024-06-28T13:25:19.317499Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/Edgar404/donut-shivi-cheques-cheques_1920/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1823\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-667eb9bd-429b6e0c7f4dfbf83b364039;7ab3c193-8b7a-4795-ba71-4168c7f7f439)\n\nRepository Not Found for url: https://huggingface.co/Edgar404/donut-shivi-cheques-cheques_1920/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#teacher import\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DonutProcessor, VisionEncoderDecoderModel\n\u001b[0;32m----> 4\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionEncoderDecoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEdgar404/donut-shivi-cheques-cheques_1920\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m teacher_processor \u001b[38;5;241m=\u001b[39m DonutProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdgar404/donut-shivi-cheques-cheques_1920\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m teacher_model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:359\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast initialization is currently not supported for VisionEncoderDecoderModel. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to slow initialization...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fast_init\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3015\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3014\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3015\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3016\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3019\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3020\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3021\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3024\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3027\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3028\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: Edgar404/donut-shivi-cheques-cheques_1920 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"],"ename":"OSError","evalue":"Edgar404/donut-shivi-cheques-cheques_1920 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","output_type":"error"}]},{"cell_type":"code","source":"from copy import deepcopy","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:22:33.208747Z","iopub.execute_input":"2024-06-28T13:22:33.209115Z","iopub.status.idle":"2024-06-28T13:22:33.215571Z","shell.execute_reply.started":"2024-06-28T13:22:33.209087Z","shell.execute_reply":"2024-06-28T13:22:33.214626Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def modify_state_dict(state_dict):\n    \"\"\"\n    Modifies the state dictionary keys by removing the 'model.' prefix.\n    \n    Args:\n    state_dict (dict): Original state dictionary with keys having 'model.' prefix.\n    \n    Returns:\n    dict: Modified state dictionary with 'model.' prefix removed from keys.\n    \"\"\"\n    modified_state_dict = {}\n    \n    for key, value in state_dict.items():\n        # Remove the 'model.' prefix from the key\n        new_key = key.replace(\"model.\", \"\", 1)\n        modified_state_dict[new_key] = value\n    \n    return modified_state_dict","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:22:33.792502Z","iopub.execute_input":"2024-06-28T13:22:33.792875Z","iopub.status.idle":"2024-06-28T13:22:33.800802Z","shell.execute_reply.started":"2024-06-28T13:22:33.792847Z","shell.execute_reply":"2024-06-28T13:22:33.799411Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# import best model\ncheckpoint = torch.load('/kaggle/working/best_model/epoch=0-step=1120.ckpt')\nbest_model = deepcopy(model)\n\nmodified_state_dict = modify_state_dict(checkpoint['state_dict'])\nbest_model.load_state_dict(modified_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:22:43.177285Z","iopub.execute_input":"2024-06-28T13:22:43.178094Z","iopub.status.idle":"2024-06-28T13:22:45.313137Z","shell.execute_reply.started":"2024-06-28T13:22:43.178061Z","shell.execute_reply":"2024-06-28T13:22:45.312038Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Function to compare two state dicts\ndef compare_state_dicts(state_dict1, state_dict2):\n    for key in state_dict1:\n        if key not in state_dict2:\n            print(f\"Key {key} not found in the second state_dict\")\n            return False\n        if not torch.equal(state_dict1[key], state_dict2[key]):\n            print(f\"Difference found in key {key}\")\n            return False\n    return True\n\n# Compare the state dicts\nif compare_state_dicts(model.state_dict(), teacher_model.state_dict()):\n    print(\"The state dicts are identical.\")\nelse:\n    print(\"The state dicts are different.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:46:40.182079Z","iopub.execute_input":"2024-06-05T09:46:40.182465Z","iopub.status.idle":"2024-06-05T09:46:40.448420Z","shell.execute_reply.started":"2024-06-05T09:46:40.182433Z","shell.execute_reply":"2024-06-05T09:46:40.443755Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"The state dicts are identical.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install jiwer\nimport jiwer","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:14:07.311060Z","iopub.execute_input":"2024-06-28T22:14:07.311446Z","iopub.status.idle":"2024-06-28T22:14:21.312539Z","shell.execute_reply.started":"2024-06-28T22:14:07.311412Z","shell.execute_reply":"2024-06-28T22:14:21.311401Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting jiwer\n  Using cached jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Using cached rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to process an image and to return the OCR\ndef process_image(image , model , processor, d_type = torch.float32):\n    \"\"\" Function that takes an image and perform an OCR using the model DonUT via the task document\n    parsing\n    \n    parameters\n    __________\n    image : a machine readable image of class PIL or numpy\"\"\"\n    \n    task_prompt = \"<s_cord-v2>\"\n    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n    outputs = model.generate(\n        pixel_values.to(device, dtype = d_type),\n        decoder_input_ids=decoder_input_ids.to(device),\n        max_length=model.decoder.config.max_position_embeddings,\n        pad_token_id=processor.tokenizer.pad_token_id,\n        eos_token_id=processor.tokenizer.eos_token_id,\n        use_cache=True,\n        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n        return_dict_in_generate=True,\n    )\n\n    sequence = processor.batch_decode(outputs.sequences)[0]\n    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n    output = processor.token2json(sequence)\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:14:21.315221Z","iopub.execute_input":"2024-06-28T22:14:21.315638Z","iopub.status.idle":"2024-06-28T22:14:21.327673Z","shell.execute_reply.started":"2024-06-28T22:14:21.315600Z","shell.execute_reply":"2024-06-28T22:14:21.326550Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def weighted_cer(prediction_dict, reference_dict):\n    \"\"\"\n    Calcul du taux d'erreur de caractère (Character Error Rate - CER)\n    entre les valeurs d'un dictionnaire de référence  et les valeurs d'un dictionnaire prédit.\n    Chaque prediction est pondérée par la taille de la chaîne de caractère de référence\n    \"\"\"\n    \n    keys  = list(reference_dict.keys())\n    weights = []\n    cers = []\n    \n    for key in keys :\n        reference = str(reference_dict[key])\n        weight  = len(reference) \n\n        if key in prediction_dict.keys():\n            prediction = str(prediction_dict[key])\n            cer = jiwer.cer(reference,prediction)\n        else :\n            cer = 1\n            \n        weights.append(weight)\n        cers.append(cer)\n        \n    weights = np.array(weights)    \n    cers = np.array(cers)\n    \n    return (weights*cers).sum() / weights.sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:14:21.329306Z","iopub.execute_input":"2024-06-28T22:14:21.329993Z","iopub.status.idle":"2024-06-28T22:14:21.351132Z","shell.execute_reply.started":"2024-06-28T22:14:21.329959Z","shell.execute_reply":"2024-06-28T22:14:21.350014Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# util function to evaluate the model\n\ndef test_model_numeric(model , processor , sample_df , d_type = torch.float32 ):\n    \n    cer_list = []\n    time_list = []\n    failed_img_list = []\n\n    for idx in tqdm(range(len(sample_df))):\n            image = sample_df['image'][idx].convert('RGB')\n            real_dict = literal_eval(sample_df['ground_truth'][idx])\n\n            start_time = time.time()\n            output = process_image(image, model , processor , d_type)\n            end_time = time.time()\n\n            predicted_dict = output\n            inference_time = end_time - start_time\n\n            cer = weighted_cer(predicted_dict,real_dict)\n\n           \n            cer_list.append(cer)\n            time_list.append(inference_time)\n            \n    return  np.array(cer_list).mean() , np.array(time_list).mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:14:21.353100Z","iopub.execute_input":"2024-06-28T22:14:21.353444Z","iopub.status.idle":"2024-06-28T22:14:21.362916Z","shell.execute_reply.started":"2024-06-28T22:14:21.353408Z","shell.execute_reply":"2024-06-28T22:14:21.361743Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sample_df = processed_dataset['validation'].select(range(20))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:14:21.364163Z","iopub.execute_input":"2024-06-28T22:14:21.364514Z","iopub.status.idle":"2024-06-28T22:14:21.376895Z","shell.execute_reply.started":"2024-06-28T22:14:21.364484Z","shell.execute_reply":"2024-06-28T22:14:21.375724Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# evaluation\ndevice = 'cuda'\nmodel.to(device)\nmean_cer , mean_time =  test_model_numeric(model.to(device) , processor , sample_df , d_type = torch.float32 )","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:14:46.253791Z","iopub.execute_input":"2024-06-28T22:14:46.254749Z","iopub.status.idle":"2024-06-28T22:15:02.919878Z","shell.execute_reply.started":"2024-06-28T22:14:46.254706Z","shell.execute_reply":"2024-06-28T22:15:02.918729Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d13ebf53057e445cb6200e8b296a5a61"}},"metadata":{}}]},{"cell_type":"code","source":"mean_cer","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:15:05.061945Z","iopub.execute_input":"2024-06-28T22:15:05.062788Z","iopub.status.idle":"2024-06-28T22:15:05.071081Z","shell.execute_reply.started":"2024-06-28T22:15:05.062753Z","shell.execute_reply":"2024-06-28T22:15:05.070081Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0.011455188550392806"},"metadata":{}}]},{"cell_type":"code","source":"mean_time","metadata":{"execution":{"iopub.status.busy":"2024-06-28T22:15:09.014733Z","iopub.execute_input":"2024-06-28T22:15:09.015135Z","iopub.status.idle":"2024-06-28T22:15:09.022907Z","shell.execute_reply.started":"2024-06-28T22:15:09.015103Z","shell.execute_reply":"2024-06-28T22:15:09.021971Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0.8051030397415161"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test du modèle","metadata":{}},{"cell_type":"code","source":"device ='cuda'","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:19:49.338570Z","iopub.status.idle":"2024-06-03T09:19:49.338867Z","shell.execute_reply.started":"2024-06-03T09:19:49.338718Z","shell.execute_reply":"2024-06-03T09:19:49.338731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DonutProcessor, VisionEncoderDecoderModel\n\nprocessor = DonutProcessor.from_pretrained(\"Edgar404/donut-shivi-recognition\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"Edgar404/donut-shivi-recognition\")\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:43:25.964473Z","iopub.execute_input":"2024-06-03T16:43:25.965242Z","iopub.status.idle":"2024-06-03T16:43:53.067517Z","shell.execute_reply.started":"2024-06-03T16:43:25.965208Z","shell.execute_reply":"2024-06-03T16:43:53.066302Z"},"trusted":true},"execution_count":130,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b4adc9836f24c34a259d86177aafee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"413e2316e50f445b8ed95421e7ae1395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75dd057f37d84342867f16218105cb68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be50272e83ce46068c34ba58509cb551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/382 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8dd536dcea4e30b988c7aa06371acc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4486b90af54c62b9a50d599faff99d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15b74497e7d042efb05ed0f67653df30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd7999de4b74cb5a92e3cdd5fd72d4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172d5d86cb5e4ec1b7ad1ec7a537b437"}},"metadata":{}}]},{"cell_type":"code","source":"# number of trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'Total Number of Parameters: {total_params/10**6}M ,\\nNumber of trainable parameters: {trainable_params/10**6}M')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:03.067639Z","iopub.execute_input":"2024-06-03T16:44:03.068055Z","iopub.status.idle":"2024-06-03T16:44:03.084316Z","shell.execute_reply.started":"2024-06-03T16:44:03.068022Z","shell.execute_reply":"2024-06-03T16:44:03.083181Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"Total Number of Parameters: 201.863288M ,\nNumber of trainable parameters: 201.863288M\n","output_type":"stream"}]},{"cell_type":"code","source":"# Defining the metrics we will use\n\ndef taux_reconnaissance(texte_reconnu, texte_original):\n    total_caracteres = len(texte_original)\n    caracteres_corrects = sum(1 for a, b in zip(texte_reconnu, texte_original) if a == b)\n    return caracteres_corrects / total_caracteres * 100\n\ndef levenshtein_distance(s1, s2):\n    \"\"\"\n    Calcul de la distance de Levenshtein entre deux chaînes.\n    \"\"\"\n    if len(s1) < len(s2):\n        return levenshtein_distance(s2, s1)\n\n    if len(s2) == 0:\n        return len(s1)\n\n    previous_row = range(len(s2) + 1)\n\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n\n    return previous_row[-1]\n\n\ndef character_error_rate(prediction, reference):\n    \"\"\"\n    Calcul du taux d'erreur de caractère (Character Error Rate - CER)\n    entre la prédiction et la référence.\n    \"\"\"\n    prediction = prediction.lower()  \n    reference = reference.lower()\n\n    distance = levenshtein_distance(prediction, reference)\n\n    max_length = max(len(prediction), len(reference))\n\n    cer = distance / max_length if max_length != 0 else 0\n\n    return cer","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:04.723187Z","iopub.execute_input":"2024-06-03T16:44:04.724039Z","iopub.status.idle":"2024-06-03T16:44:04.739394Z","shell.execute_reply.started":"2024-06-03T16:44:04.723990Z","shell.execute_reply":"2024-06-03T16:44:04.738128Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"def weighted_cer(prediction_dict, reference_dict):\n    \"\"\"\n    Calcul du taux d'erreur de caractère (Character Error Rate - CER)\n    entre les valeurs d'un dictionnaire de référence  et les valeurs d'un dictionnaire prédit.\n    Chaque prediction est pondérée par la taille de la chaîne de caractère de référence\n    \"\"\"\n    \n    keys  = list(reference_dict.keys())\n    weights = []\n    cers = []\n    \n    for key in keys :\n        reference = str(reference_dict[key])\n        weight  = len(reference) \n\n        if key in prediction_dict.keys():\n            prediction = str(prediction_dict[key])\n            cer = character_error_rate(reference,prediction)\n        else :\n            cer = 1\n            \n        weights.append(weight)\n        cers.append(cer)\n        \n    weights = np.array(weights)    \n    cers = np.array(cers)\n    \n    return (weights*cers).sum() / weights.sum()\n\n\n\n\ndef weighted_tr(prediction_dict, reference_dict):\n    \"\"\"\n    Calcul du taux de reconnaissance\n    entre les valeurs d'un dictionnaire de référence  et les valeurs d'un dictionnaire prédit.\n    Chaque prediction est pondérée par la taille de la chaîne de caractère de référence\n    \"\"\"\n    keys  = list(reference_dict.keys())\n    weights = []\n    trs = []\n    \n    for key in keys :\n        \n        reference = str(reference_dict[key])\n        weight  = len(reference)\n\n        if key in prediction_dict.keys():\n            \n            prediction = str(prediction_dict[key])\n            tr = taux_reconnaissance(reference,prediction)\n        else :\n            tr = 0\n            \n        weights.append(weight)\n        trs.append(tr)\n        \n    weights = np.array(weights)  \n    trs = np.array(trs)\n    \n    \n    return (weights*trs).sum() / weights.sum()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:05.704987Z","iopub.execute_input":"2024-06-03T16:44:05.705873Z","iopub.status.idle":"2024-06-03T16:44:05.720296Z","shell.execute_reply.started":"2024-06-03T16:44:05.705838Z","shell.execute_reply":"2024-06-03T16:44:05.718951Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"def test_model_numeric(model , processor , sample_df , d_type = torch.float32 ):\n    \n    tr_list = []\n    cer_list = []\n    time_list = []\n    failed_img_list = []\n    i = 0\n\n    for j in tqdm(range(len(sample_df))):\n        try :\n            idx =  random.randint(0, len(sample_df))\n            image = sample_df['image'][idx].convert('RGB')\n            real_dict = sample_df['text'][idx]\n\n            start_time = time.time()\n            output = process_image(image, model , processor , d_type)\n            end_time = time.time()\n\n            predicted_dict = output\n            inference_time = end_time - start_time\n\n            tr = weighted_tr(predicted_dict,real_dict)\n            cer = weighted_cer(predicted_dict,real_dict)\n\n            tr_list.append(tr)\n            cer_list.append(cer)\n            time_list.append(inference_time)\n    \n        except Exception :\n            print(f'Broken for {output}')\n            failed_img_list.append(image)\n            i+=1\n            continue\n            \n    return np.array(tr_list).mean() , np.array(cer_list).mean() , np.array(time_list).mean() , i","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:06.024375Z","iopub.execute_input":"2024-06-03T16:44:06.024758Z","iopub.status.idle":"2024-06-03T16:44:06.037336Z","shell.execute_reply.started":"2024-06-03T16:44:06.024729Z","shell.execute_reply":"2024-06-03T16:44:06.035988Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"def get_size_of_model(model):\n    \"\"\" Returns the size of the model in Mo.\n    \n    Args:\n        model: model whose size needs to be determined\n\n    \"\"\"\n    torch.save(model.state_dict(), \"temp.p\")\n    size = os.path.getsize(\"temp.p\")/1e6\n    os.remove('temp.p')\n    \n    return size","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:06.602257Z","iopub.execute_input":"2024-06-03T16:44:06.603005Z","iopub.status.idle":"2024-06-03T16:44:06.610563Z","shell.execute_reply.started":"2024-06-03T16:44:06.602968Z","shell.execute_reply":"2024-06-03T16:44:06.609103Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"model_size = get_size_of_model(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:07.313150Z","iopub.execute_input":"2024-06-03T16:44:07.314009Z","iopub.status.idle":"2024-06-03T16:44:08.897272Z","shell.execute_reply.started":"2024-06-03T16:44:07.313972Z","shell.execute_reply":"2024-06-03T16:44:08.895782Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"numeric_sample_df = load_dataset('podbilabs/sroie-donut' , split = 'validation')\n\ndef preprocess_dataset_numeric(sample):\n  text =  json.loads(sample['ground_truth'])\n  details = text['gt_parse']\n  return {'image':sample['image'] ,'text': details }","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:44:16.537769Z","iopub.execute_input":"2024-06-03T16:44:16.538201Z","iopub.status.idle":"2024-06-03T16:44:56.173068Z","shell.execute_reply.started":"2024-06-03T16:44:16.538165Z","shell.execute_reply":"2024-06-03T16:44:56.171992Z"},"trusted":true},"execution_count":138,"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d98567fad8a4646a87e7cae3aa623b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/127 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4436551afe84ec9b30012d7da629993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/348 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4db0adacc8e48cf9c11dee750678563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/503 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e785e3868d9742a3a8ad0af711d8790e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/129 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b91860eda1f849b683e738ac480ac23c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/350 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1caad24696c346e7971436e32e4c8353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2868e620cb04fae9adaf3057415e660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/126 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2459d53d128a441480925aa918637c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/347 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91694178d25347bd94e530ddb67ba1de"}},"metadata":{}}]},{"cell_type":"code","source":"numeric_sample_df = numeric_sample_df.map(preprocess_dataset_numeric , remove_columns = ['ground_truth'])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:52:50.520719Z","iopub.execute_input":"2024-04-21T20:52:50.521365Z","iopub.status.idle":"2024-04-21T20:52:50.543691Z","shell.execute_reply.started":"2024-04-21T20:52:50.521331Z","shell.execute_reply":"2024-04-21T20:52:50.542799Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"mean_tr , men_cr , mean_time , i = test_model_numeric(model.to(device) ,processor ,numeric_sample_df )","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:52:50.544847Z","iopub.execute_input":"2024-04-21T20:52:50.545193Z","iopub.status.idle":"2024-04-21T21:02:11.358082Z","shell.execute_reply.started":"2024-04-21T20:52:50.545161Z","shell.execute_reply":"2024-04-21T21:02:11.356897Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/126 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99b33f815a634d1595f7095d2d1606ec"}},"metadata":{}},{"name":"stdout","text":"Broken for {'total': '10.40', 'date': '21/09/2017', 'company': 'HON HWA HARDWARE TRADING', 'address': 'NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"from prettytable import PrettyTable\ntable = PrettyTable()\ntable.field_names = ['model_name','mean_tr','mean_cer','storage (M)','% parameters trained','mean_inference_time(s)','Broken examples']","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:02:11.360515Z","iopub.execute_input":"2024-04-21T21:02:11.360971Z","iopub.status.idle":"2024-04-21T21:02:11.366183Z","shell.execute_reply.started":"2024-04-21T21:02:11.360942Z","shell.execute_reply":"2024-04-21T21:02:11.365264Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"table.add_row(['Donut_combined_float32(numeric_eval_set)', mean_tr , men_cr ,model_size ,100 ,mean_time,i ])\nprint(table)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:05:57.936764Z","iopub.execute_input":"2024-04-21T21:05:57.937652Z","iopub.status.idle":"2024-04-21T21:05:57.943503Z","shell.execute_reply.started":"2024-04-21T21:05:57.937618Z","shell.execute_reply":"2024-04-21T21:05:57.942525Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"+------------------------------------------+-------------------+---------------------+-------------+----------------------+------------------------+-----------------+\n|                model_name                |      mean_tr      |       mean_cer      | storage (M) | % parameters trained | mean_inference_time(s) | Broken examples |\n+------------------------------------------+-------------------+---------------------+-------------+----------------------+------------------------+-----------------+\n| Donut_combined_float32(numeric_eval_set) | 72.67797453681499 | 0.11487697827876517 |  809.256887 |         100          |   0.5071631832122803   |        1        |\n+------------------------------------------+-------------------+---------------------+-------------+----------------------+------------------------+-----------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"#handwritten sample\nfrom datasets import load_dataset\ndataset_1 = load_dataset('shivi/cheques_sample_data' , split = 'validation[:126]' )\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:45:06.321754Z","iopub.execute_input":"2024-06-03T16:45:06.322818Z","iopub.status.idle":"2024-06-03T16:45:07.240046Z","shell.execute_reply.started":"2024-06-03T16:45:06.322773Z","shell.execute_reply":"2024-06-03T16:45:07.238818Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"def preprocess_dataset(sample):\n  text =  json.loads(sample['ground_truth'])\n  details = text['gt_parse']['cheque_details']\n  target = details[0]\n  for detail in details :\n    target.update(detail)\n\n  return {'image':sample['image'] ,'text': target }\n\nprocessed_dataset = dataset_1.map(preprocess_dataset , remove_columns = ['ground_truth'])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:45:07.242465Z","iopub.execute_input":"2024-06-03T16:45:07.242930Z","iopub.status.idle":"2024-06-03T16:45:07.257711Z","shell.execute_reply.started":"2024-06-03T16:45:07.242880Z","shell.execute_reply":"2024-06-03T16:45:07.256597Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"sample_df = processed_dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:45:12.533928Z","iopub.execute_input":"2024-06-03T16:45:12.534342Z","iopub.status.idle":"2024-06-03T16:45:12.540614Z","shell.execute_reply.started":"2024-06-03T16:45:12.534308Z","shell.execute_reply":"2024-06-03T16:45:12.539489Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"mean_tr , men_cr , mean_time , i = test_model_numeric(model.to(device) ,processor ,sample_df )","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:45:13.993312Z","iopub.execute_input":"2024-06-03T16:45:13.994236Z","iopub.status.idle":"2024-06-03T16:46:13.006551Z","shell.execute_reply.started":"2024-06-03T16:45:13.994197Z","shell.execute_reply":"2024-06-03T16:46:13.005262Z"},"trusted":true},"execution_count":142,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/126 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2936fc827334b6aa811df2bedaa9d35"}},"metadata":{}}]},{"cell_type":"code","source":"from prettytable import PrettyTable\ntable = PrettyTable()\ntable.field_names = ['model_name','mean_tr','mean_cer','storage (M)','% parameters trained','mean_inference_time(s)','Broken examples']","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:46:54.224782Z","iopub.execute_input":"2024-06-03T16:46:54.225286Z","iopub.status.idle":"2024-06-03T16:46:54.232559Z","shell.execute_reply.started":"2024-06-03T16:46:54.225250Z","shell.execute_reply":"2024-06-03T16:46:54.231069Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"table.add_row(['Donut_combined_float32(handwritten_eval_set)', mean_tr , men_cr ,model_size ,100 ,mean_time,i ])\nprint(table)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:46:55.231246Z","iopub.execute_input":"2024-06-03T16:46:55.231635Z","iopub.status.idle":"2024-06-03T16:46:55.241393Z","shell.execute_reply.started":"2024-06-03T16:46:55.231605Z","shell.execute_reply":"2024-06-03T16:46:55.240072Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"+----------------------------------------------+-------------------+----------------------+-------------+----------------------+------------------------+-----------------+\n|                  model_name                  |      mean_tr      |       mean_cer       | storage (M) | % parameters trained | mean_inference_time(s) | Broken examples |\n+----------------------------------------------+-------------------+----------------------+-------------+----------------------+------------------------+-----------------+\n| Donut_combined_float32(handwritten_eval_set) | 82.21799329550355 | 0.031892813541771516 |  809.224119 |         100          |  0.38538669215308297   |        0        |\n+----------------------------------------------+-------------------+----------------------+-------------+----------------------+------------------------+-----------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('test_combined.csv', 'w', newline='') as f_output:\n    f_output.write(table.get_csv_string())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:20:04.813541Z","iopub.execute_input":"2024-04-21T21:20:04.813932Z","iopub.status.idle":"2024-04-21T21:20:04.819297Z","shell.execute_reply.started":"2024-04-21T21:20:04.813902Z","shell.execute_reply":"2024-04-21T21:20:04.818274Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import VisionEncoderDecoderModel, DonutProcessor\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the initial model and processor\nmodel_name = \"Edgar404/donut-shivi-cheques_320\"\ninitial_model = VisionEncoderDecoderModel.from_pretrained(model_name)\ninitial_processor = DonutProcessor.from_pretrained(model_name)\n\n# Move the model to the correct device\ninitial_model.to(device)\n\n# Set the model to evaluation mode\ninitial_model.eval()\n\n# Example input data (replace 'your_image' with actual image data)\nyour_image = sample_df[0]['image']  # Load your image here\ninput_data = initial_processor(images=your_image, return_tensors=\"pt\").to(device)\n\n# Get output from the initial model\nwith torch.no_grad():\n    initial_outputs = initial_model(**input_data)\n\n# Save the model and processor locally\nsave_path = \"./temp_model\"\ninitial_model.save_pretrained(save_path)\ninitial_processor.save_pretrained(save_path)\n\n# Load the saved model and processor\nloaded_model = VisionEncoderDecoderModel.from_pretrained(save_path)\nloaded_processor = DonutProcessor.from_pretrained(save_path)\n\n# Move the loaded model to the correct device\nloaded_model.to(device)\n\n# Set the loaded model to evaluation mode\nloaded_model.eval()\n\n# Get output from the loaded model using the same input data\nwith torch.no_grad():\n    loaded_outputs = loaded_model(**input_data)\n\n# Function to compare state dicts\ndef compare_state_dicts(state_dict1, state_dict2):\n    for key in state_dict1:\n        if key not in state_dict2:\n            print(f\"Key {key} not found in the second state_dict\")\n            return False\n        if not torch.equal(state_dict1[key], state_dict2[key]):\n            print(f\"Difference found in key {key}\")\n            return False\n    return True\n\n# Compare the state dicts\nif compare_state_dicts(initial_model.state_dict(), loaded_model.state_dict()):\n    print(\"The state dicts are identical.\")\nelse:\n    print(\"The state dicts are different.\")\n\n# Inspect and compare the outputs\nprint(\"Initial Outputs:\", initial_outputs)\nprint(\"Loaded Outputs:\", loaded_outputs)\n\n# Function to inspect the outputs\ndef inspect_outputs(output1, output2):\n    for key in output1:\n        if key in output2:\n            print(f\"Output for {key} - Initi\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:50:42.676053Z","iopub.execute_input":"2024-06-03T16:50:42.676481Z","iopub.status.idle":"2024-06-03T16:50:42.694625Z","shell.execute_reply.started":"2024-06-03T16:50:42.676443Z","shell.execute_reply":"2024-06-03T16:50:42.692776Z"},"trusted":true},"execution_count":146,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[146], line 70\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Output for {key} - Initi\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 70)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 70) (1183123720.py, line 70)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}