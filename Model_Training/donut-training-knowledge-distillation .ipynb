{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1400106,"sourceType":"datasetVersion","datasetId":818027}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q jiwer mlflow datasets peft prettytable evaluate optuna bitsandbytes accelerate --upgrade ","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:34:57.434938Z","iopub.execute_input":"2024-07-01T12:34:57.435682Z","iopub.status.idle":"2024-07-01T12:35:04.400591Z","shell.execute_reply.started":"2024-07-01T12:34:57.435649Z","shell.execute_reply":"2024-07-01T12:35:04.399426Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\n\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install  -q transformers==4.40.0","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:35:04.402918Z","iopub.execute_input":"2024-07-01T12:35:04.403856Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader , Dataset\nfrom torchvision.utils import make_grid\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport os\nfrom pathlib import Path\nfrom glob import glob\nfrom tqdm.auto import tqdm\nimport tempfile\n\nimport random\nimport time\nimport cv2\nimport sys\nfrom prettytable import PrettyTable\n\nimport re\nimport string\nimport shutil\n\nimport optuna \nfrom datasets import load_dataset , concatenate_datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:39:36.830883Z","iopub.execute_input":"2024-06-29T10:39:36.831266Z","iopub.status.idle":"2024-06-29T10:39:39.263055Z","shell.execute_reply.started":"2024-06-29T10:39:36.831237Z","shell.execute_reply":"2024-06-29T10:39:39.262273Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Handwritten Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_1 = load_dataset('shivi/cheques_sample_data' )","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:39:54.093713Z","iopub.execute_input":"2024-06-29T10:39:54.094909Z","iopub.status.idle":"2024-06-29T10:40:08.860737Z","shell.execute_reply.started":"2024-06-29T10:39:54.094867Z","shell.execute_reply":"2024-06-29T10:40:08.860018Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/540 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c599372fa4464820b5293dc17bdfdc86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/41.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f906dd77bc0d418cbf0a8ced8a821daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb15064cec97447bbfb3ec337a7ea191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.87M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb98a29341ad4753b55717066961ea49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eaf58fdacd64b09aaf92d648c6df338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5375957cbd414bf288cfac5862408899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d1d4d20f42478ba15acc6fd09f07ee"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_dataset(sample):\n  text =  json.loads(sample['ground_truth'])\n  details = text['gt_parse']['cheque_details']\n  target = details[0]\n  for detail in details :\n    target.update(detail)\n\n  return {'image':sample['image'] ,'ground_truth': str(target) }","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:40:17.724900Z","iopub.execute_input":"2024-06-29T10:40:17.725615Z","iopub.status.idle":"2024-06-29T10:40:17.731430Z","shell.execute_reply.started":"2024-06-29T10:40:17.725583Z","shell.execute_reply":"2024-06-29T10:40:17.730141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"processed_dataset = dataset_1.map(preprocess_dataset , remove_columns = ['ground_truth'])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:40:41.171064Z","iopub.execute_input":"2024-06-29T10:40:41.171708Z","iopub.status.idle":"2024-06-29T10:40:47.268632Z","shell.execute_reply.started":"2024-06-29T10:40:41.171664Z","shell.execute_reply":"2024-06-29T10:40:47.267663Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec8f09bebeb4a3387b97b73bab9bd4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f04b9685bb4be6ad1ac15c7531f87e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f3a0d757b64e7cbab9a05a90075186"}},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset['test']= processed_dataset['test'].select(range(20))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:40:51.780119Z","iopub.execute_input":"2024-06-29T10:40:51.780528Z","iopub.status.idle":"2024-06-29T10:40:51.788108Z","shell.execute_reply.started":"2024-06-29T10:40:51.780498Z","shell.execute_reply":"2024-06-29T10:40:51.787250Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Model Config","metadata":{}},{"cell_type":"code","source":"from ast import literal_eval","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:40:56.231229Z","iopub.execute_input":"2024-06-29T10:40:56.232155Z","iopub.status.idle":"2024-06-29T10:40:56.236446Z","shell.execute_reply.started":"2024-06-29T10:40:56.232113Z","shell.execute_reply":"2024-06-29T10:40:56.235507Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderConfig , BitsandBytesConfig\n\nimage_size = [500,500]\nmax_length = 512\n\n# update image_size of the encoder\n# during pre-training, a larger image size was used\nconfig = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\nconfig.encoder.image_size = image_size # (height, width)\n# update max_length of the decoder (for generation)\nconfig.decoder.max_length = max_length","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:32:05.685619Z","iopub.execute_input":"2024-06-29T07:32:05.685957Z","iopub.status.idle":"2024-06-29T07:32:06.244814Z","shell.execute_reply.started":"2024-06-29T07:32:05.685930Z","shell.execute_reply":"2024-06-29T07:32:06.243882Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576f001aebe947028636276d37be3d70"}},"metadata":{}}]},{"cell_type":"code","source":"quantization_config = BitsandBytesConfig()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token = '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')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:32:06.246059Z","iopub.execute_input":"2024-06-29T07:32:06.246410Z","iopub.status.idle":"2024-06-29T07:32:06.465995Z","shell.execute_reply.started":"2024-06-29T07:32:06.246376Z","shell.execute_reply":"2024-06-29T07:32:06.464756Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DonutProcessor, VisionEncoderDecoderModel\n\nprocessor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:32:06.467330Z","iopub.execute_input":"2024-06-29T07:32:06.467659Z","iopub.status.idle":"2024-06-29T07:32:50.912692Z","shell.execute_reply.started":"2024-06-29T07:32:06.467621Z","shell.execute_reply":"2024-06-29T07:32:50.911803Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-06-29 07:32:08.421707: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 07:32:08.421816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 07:32:08.531491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/362 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad4d8633e76a4985931ae7f66efe6485"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/518 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34734a31401a4121abe8bd3f92120cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0cbee4202754e17bea8da30e9af3093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e7c6c0fa1e4a10a0c1388417e85e46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/71.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3df66d1cb3c430fa7b959e3eac3a025"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7089b4bbec14f63abd19a9d7d571bab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78b44e3e22f48f6bfb2b03180d8bf7a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\nteacher_model = VisionEncoderDecoderModel.from_pretrained(\"Edgar404/donut-shivi-cheques_1920\")\nteacher_processor = DonutProcessor.from_pretrained(\"Edgar404/donut-shivi-cheques_1920\")\nteacher_model.eval()\nteacher_model.to('cuda')\nloss_fct = CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:32:50.914170Z","iopub.execute_input":"2024-06-29T07:32:50.914583Z","iopub.status.idle":"2024-06-29T07:33:32.976361Z","shell.execute_reply.started":"2024-06-29T07:32:50.914544Z","shell.execute_reply":"2024-06-29T07:33:32.975334Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a536ad5d4ea4a33b7eb67541ad00092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681172c36dd248b6a3cf82814e4d4171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27a9d1352eb4add800ad4538665d2f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e11ba04ca9e469abae2ac317f4c6a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365e07b1f65540ea94f2bdc4d28deca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99450ad061f4de583f48dc97c99b6a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853e6c6929a34d88a42ffc8c4db4ec61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/382 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e0775b516241f998fc51eaeab6743f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4f3d15a9fc4be1b9899ee13bffd40d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"for param in teacher_model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:32.977735Z","iopub.execute_input":"2024-06-29T07:33:32.978109Z","iopub.status.idle":"2024-06-29T07:33:32.984957Z","shell.execute_reply.started":"2024-06-29T07:33:32.978072Z","shell.execute_reply":"2024-06-29T07:33:32.983921Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from typing import Any, List, Tuple\nadded_tokens = []\n\n\nclass DonutDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for Donut. This class takes a HuggingFace Dataset as input.\n\n    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n    and it will be converted into pixel_values (vectorized image) and labels (input_ids of the tokenized string).\n\n    Args:\n        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n        max_length: the max number of tokens for the target sequences\n        split: whether to load \"train\", \"validation\" or \"test\" split\n        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n        task_start_token: the special token to be fed to the decoder to conduct the target task\n        prompt_end_token: the special token at the end of the sequences\n        sort_json_key: whether or not to sort the JSON keys\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        max_length: int,\n        split: str = \"train\",\n        ignore_id: int = -100,\n        task_start_token: str = \"<s>\",\n        prompt_end_token: str = None,\n        sort_json_key: bool = True,\n    ):\n        super().__init__()\n\n        self.max_length = max_length\n        self.split = split\n        self.ignore_id = ignore_id\n        self.task_start_token = task_start_token\n        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n        self.sort_json_key = sort_json_key\n\n        self.dataset = dataset[self.split]\n        self.dataset_length = len(self.dataset)\n\n        self.gt_token_sequences = []\n        for sample in self.dataset:\n            ground_truth = literal_eval(sample[\"ground_truth\"])\n            assert isinstance(ground_truth, dict)\n            gt_jsons = [ground_truth]\n\n            self.gt_token_sequences.append(\n                [\n                    self.json2token(\n                        gt_jsons,\n                        update_special_tokens_for_json_key=self.split == \"train\",\n                        sort_json_key=self.sort_json_key,\n                    )\n                    + processor.tokenizer.eos_token\n\n                ]\n            )\n\n        self.add_tokens([self.task_start_token, self.prompt_end_token])\n        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n\n    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n        \"\"\"\n        Convert an ordered JSON object into a token sequence\n        \"\"\"\n        if type(obj) == dict:\n                output = \"\"\n                if sort_json_key:\n                    keys = sorted(obj.keys(), reverse=True)\n                else:\n                    keys = obj.keys()\n\n                for k in keys:\n                    if update_special_tokens_for_json_key:\n                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n                    output += (\n                        fr\"<s_{k}>\"\n                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n                        + fr\"</s_{k}>\"\n                    )\n                return output\n\n        elif type(obj) == list:\n            return r\"<sep/>\".join(\n                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n            )\n        else:\n            obj = str(obj)\n            if f\"<{obj}/>\" in added_tokens:\n                obj = f\"<{obj}/>\"  # for categorical special tokens\n            return obj\n\n    def add_tokens(self, list_of_tokens: List[str]):\n        \"\"\"\n        Add special tokens to tokenizer and resize the token embeddings of the decoder\n        \"\"\"\n        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n        if newly_added_num > 0:\n            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n            added_tokens.extend(list_of_tokens)\n\n    def __len__(self) -> int:\n        return self.dataset_length\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Load image from image_path of given dataset_path and convert into input_tensor and labels\n        Convert gt data into input_ids (tokenized string)\n        Returns:\n            input_tensor : preprocessed image\n            input_ids : tokenized gt_data\n            labels : masked labels (model doesn't need to predict prompt and pad token)\n        \"\"\"\n        sample = self.dataset[idx]\n\n        # inputs\n        pixel_values_student = processor(sample[\"image\"].convert('RGB'), random_padding= self.split == \"train\", return_tensors=\"pt\").pixel_values\n        pixel_values_student = pixel_values_student.squeeze()\n        \n        pixel_values_teacher = teacher_processor(sample[\"image\"].convert('RGB'), random_padding= self.split == \"train\", return_tensors=\"pt\").pixel_values\n        pixel_values_teacher = pixel_values_teacher.squeeze()\n\n        # targets\n        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n        input_ids = processor.tokenizer(\n            target_sequence,\n            add_special_tokens=False,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )[\"input_ids\"].squeeze(0)\n\n        labels = input_ids.clone()\n        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n        return pixel_values_student, pixel_values_teacher, labels, target_sequence","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:32.986317Z","iopub.execute_input":"2024-06-29T07:33:32.986807Z","iopub.status.idle":"2024-06-29T07:33:33.009599Z","shell.execute_reply.started":"2024-06-29T07:33:32.986780Z","shell.execute_reply":"2024-06-29T07:33:33.008795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"processor.image_processor.size = image_size[::-1] # should be (width, height)\nprocessor.image_processor.do_align_long_axis = False\n\ntrain_dataset = DonutDataset(processed_dataset, max_length=max_length,\n                             split=\"train\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n                             sort_json_key=True, # cord dataset is preprocessed, so no need for this\n                             )\n\nval_dataset = DonutDataset(processed_dataset, max_length=max_length,\n                             split=\"test\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n                             sort_json_key=True, # cord dataset is preprocessed, so no need for this\n                             )","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:33.010702Z","iopub.execute_input":"2024-06-29T07:33:33.011008Z","iopub.status.idle":"2024-06-29T07:33:44.036785Z","shell.execute_reply.started":"2024-06-29T07:33:33.010971Z","shell.execute_reply":"2024-06-29T07:33:44.036005Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"len(added_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.037864Z","iopub.execute_input":"2024-06-29T07:33:44.038126Z","iopub.status.idle":"2024-06-29T07:33:44.047231Z","shell.execute_reply.started":"2024-06-29T07:33:44.038103Z","shell.execute_reply":"2024-06-29T07:33:44.046429Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"print(added_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.048485Z","iopub.execute_input":"2024-06-29T07:33:44.049374Z","iopub.status.idle":"2024-06-29T07:33:44.053994Z","shell.execute_reply.started":"2024-06-29T07:33:44.049338Z","shell.execute_reply":"2024-06-29T07:33:44.053149Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['<s_payee_name>', '</s_payee_name>', '<s_cheque_date>', '</s_cheque_date>', '<s_bank_name>', '</s_bank_name>', '<s_amt_in_words>', '</s_amt_in_words>', '<s_amt_in_figures>', '</s_amt_in_figures>', '<s_cord-v2>', '<s_cord-v2>']\n","output_type":"stream"}]},{"cell_type":"code","source":"# the vocab size attribute stays constants (might be a bit unintuitive - but doesn't include special tokens)\nprint(\"Original number of tokens:\", processor.tokenizer.vocab_size)\nprint(\"Number of tokens after adding special tokens:\", len(processor.tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.055315Z","iopub.execute_input":"2024-06-29T07:33:44.055691Z","iopub.status.idle":"2024-06-29T07:33:44.073809Z","shell.execute_reply.started":"2024-06-29T07:33:44.055659Z","shell.execute_reply":"2024-06-29T07:33:44.072881Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Original number of tokens: 57522\nNumber of tokens after adding special tokens: 57536\n","output_type":"stream"}]},{"cell_type":"code","source":"processor.decode([57527])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.075086Z","iopub.execute_input":"2024-06-29T07:33:44.075557Z","iopub.status.idle":"2024-06-29T07:33:44.083110Z","shell.execute_reply.started":"2024-06-29T07:33:44.075518Z","shell.execute_reply":"2024-06-29T07:33:44.082292Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'<s_cheque_date>'"},"metadata":{}}]},{"cell_type":"code","source":"pixel_values, _, labels, target_sequence = train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.084261Z","iopub.execute_input":"2024-06-29T07:33:44.084715Z","iopub.status.idle":"2024-06-29T07:33:44.346728Z","shell.execute_reply.started":"2024-06-29T07:33:44.084682Z","shell.execute_reply":"2024-06-29T07:33:44.345707Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(pixel_values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.348128Z","iopub.execute_input":"2024-06-29T07:33:44.348848Z","iopub.status.idle":"2024-06-29T07:33:44.354210Z","shell.execute_reply.started":"2024-06-29T07:33:44.348811Z","shell.execute_reply":"2024-06-29T07:33:44.353308Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"torch.Size([3, 500, 500])\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's print the labels (the first 30 token ID's)\nfor id in labels.tolist()[:30]:\n  if id != -100:\n    print(processor.decode([id]))\n  else:\n    print(id)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.355467Z","iopub.execute_input":"2024-06-29T07:33:44.355792Z","iopub.status.idle":"2024-06-29T07:33:44.364208Z","shell.execute_reply.started":"2024-06-29T07:33:44.355759Z","shell.execute_reply":"2024-06-29T07:33:44.363343Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"<s_payee_name>\nEd\nmee\nP\nelle\ntier\n</s_payee_name>\n<s_cheque_date>\n06\n/\n05\n/\n22\n</s_cheque_date>\n<s_bank_name>\nA\nXI\nS\nBA\nNK\n</s_bank_name>\n<s_amt_in_words>\nTh\nre\ne\nTho\nus\nand\nSe\nven\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's check the corresponding target sequence, as a string\nprint(target_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.365381Z","iopub.execute_input":"2024-06-29T07:33:44.365763Z","iopub.status.idle":"2024-06-29T07:33:44.374058Z","shell.execute_reply.started":"2024-06-29T07:33:44.365739Z","shell.execute_reply":"2024-06-29T07:33:44.373115Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"<s_payee_name>Edmee Pelletier</s_payee_name><s_cheque_date>06/05/22</s_cheque_date><s_bank_name>AXIS BANK</s_bank_name><s_amt_in_words>Three Thousand Seven Hundred and Fifty Five</s_amt_in_words><s_amt_in_figures>3755</s_amt_in_figures></s>\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.pad_token_id = processor.tokenizer.pad_token_id\nmodel.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_cord-v2>'])[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.375262Z","iopub.execute_input":"2024-06-29T07:33:44.375873Z","iopub.status.idle":"2024-06-29T07:33:44.383790Z","shell.execute_reply.started":"2024-06-29T07:33:44.375848Z","shell.execute_reply":"2024-06-29T07:33:44.383012Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# sanity check\nprint(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\nprint(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.385653Z","iopub.execute_input":"2024-06-29T07:33:44.385914Z","iopub.status.idle":"2024-06-29T07:33:44.393557Z","shell.execute_reply.started":"2024-06-29T07:33:44.385891Z","shell.execute_reply":"2024-06-29T07:33:44.392698Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Pad token ID: <pad>\nDecoder start token ID: <s_cord-v2>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating pytorch dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# feel free to increase the batch size if you have a lot of memory\n# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\nval_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.394742Z","iopub.execute_input":"2024-06-29T07:33:44.395026Z","iopub.status.idle":"2024-06-29T07:33:44.402984Z","shell.execute_reply.started":"2024-06-29T07:33:44.395001Z","shell.execute_reply":"2024-06-29T07:33:44.401992Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\npixel_values, _, labels, target_sequences = batch\nprint(pixel_values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:44.409686Z","iopub.execute_input":"2024-06-29T07:33:44.409954Z","iopub.status.idle":"2024-06-29T07:33:46.610496Z","shell.execute_reply.started":"2024-06-29T07:33:44.409923Z","shell.execute_reply":"2024-06-29T07:33:46.609358Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2, 3, 500, 500])\n","output_type":"stream"}]},{"cell_type":"code","source":"for id in labels[0].squeeze().tolist()[:30]:\n  if id != -100:\n    print(processor.decode([id]))\n  else:\n    print(id)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:46.613882Z","iopub.execute_input":"2024-06-29T07:33:46.614198Z","iopub.status.idle":"2024-06-29T07:33:46.621556Z","shell.execute_reply.started":"2024-06-29T07:33:46.614170Z","shell.execute_reply":"2024-06-29T07:33:46.620569Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"<s_payee_name>\nSid\nney\nLa\npoint\ne\n</s_payee_name>\n<s_cheque_date>\n06\n/\n05\n/\n22\n</s_cheque_date>\n<s_bank_name>\nIC\nICI\nBank\n</s_bank_name>\n<s_amt_in_words>\nTwo\nTho\nus\nand\nSix\nHund\nred\nand\nFi\nft\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(val_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:46.622781Z","iopub.execute_input":"2024-06-29T07:33:46.623090Z","iopub.status.idle":"2024-06-29T07:33:46.630848Z","shell.execute_reply.started":"2024-06-29T07:33:46.623064Z","shell.execute_reply":"2024-06-29T07:33:46.629950Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"2800\n20\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's check the first validation batch\nbatch = next(iter(val_dataloader))\npixel_values, _, labels, target_sequences = batch\nprint(pixel_values.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:46.631893Z","iopub.execute_input":"2024-06-29T07:33:46.632182Z","iopub.status.idle":"2024-06-29T07:33:48.797575Z","shell.execute_reply.started":"2024-06-29T07:33:46.632157Z","shell.execute_reply":"2024-06-29T07:33:48.796386Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2, 3, 500, 500])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(target_sequences[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:48.799235Z","iopub.execute_input":"2024-06-29T07:33:48.799691Z","iopub.status.idle":"2024-06-29T07:33:48.805265Z","shell.execute_reply.started":"2024-06-29T07:33:48.799645Z","shell.execute_reply":"2024-06-29T07:33:48.804251Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<s_payee_name>Seymour Patenaude</s_payee_name><s_cheque_date>06/05/22</s_cheque_date><s_bank_name>Canara Bank</s_bank_name><s_amt_in_words>Two Thousand Thre Hundred and Twenty Two</s_amt_in_words><s_amt_in_figures>2322</s_amt_in_figures></s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define LightningModule","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport re\nfrom nltk import edit_distance\nimport numpy as np\nimport math\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import LambdaLR\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\nclass DonutModelPLModule(pl.LightningModule):\n    def __init__(self, config, processor, model):\n        super().__init__()\n        self.config = config\n        self.processor = processor\n        self.model = model\n        self.temperature = 1.0\n        self.alpha = 0.9 \n        self.soft_loss_scale = 1e-4  \n\n    def training_step(self, batch, batch_idx):\n        pixel_values_student, pixel_values_teacher, labels, _ = batch\n        student_outputs = self.model(pixel_values_student, labels=labels)\n        \n        with torch.no_grad():\n            teacher_outputs = teacher_model(pixel_values_teacher, labels=labels)\n            \n        hard_loss = student_outputs.loss\n        \n        # Calculate soft label loss\n        soft_loss = nn.KLDivLoss(reduction='batchmean')(nn.functional.log_softmax(student_outputs.logits / self.temperature, dim=1),\n                                   nn.functional.softmax(teacher_outputs.logits / self.temperature, dim=1)) * (self.temperature * self.temperature)\n        \n        soft_loss *= self.soft_loss_scale\n        # Combine losses\n        loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n        self.log(\"hard_loss\", hard_loss)\n        self.log(\"soft_loss\", soft_loss)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx, dataset_idx=0):\n        pixel_values, _, labels, answers = batch\n        batch_size = pixel_values.shape[0]\n        # we feed the prompt to the model\n        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n        \n        outputs = self.model.generate(pixel_values,\n                                   decoder_input_ids=decoder_input_ids,\n                                   max_length=max_length,\n                                   early_stopping=True,\n                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n                                   use_cache=True,\n                                   num_beams=1,\n                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n                                   return_dict_in_generate=True,)\n    \n        predictions = []\n        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n            predictions.append(seq)\n\n        scores = []\n        for pred, answer in zip(predictions, answers):\n            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n            # NOT NEEDED ANYMORE\n            # answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n\n            if self.config.get(\"verbose\", False) and len(scores) == 1:\n                print(f\"Prediction: {pred}\")\n                print(f\"    Answer: {answer}\")\n                print(f\" Normed ED: {scores[0]}\")\n\n        self.log(\"val_edit_distance\", np.mean(scores))\n        \n        return scores\n\n    def configure_optimizers(self):\n        # you could also add a learning rate scheduler if you want\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n    \n        return optimizer\n\n    def train_dataloader(self):\n        return train_dataloader\n\n    def val_dataloader(self):\n        return val_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:48.806743Z","iopub.execute_input":"2024-06-29T07:33:48.807055Z","iopub.status.idle":"2024-06-29T07:33:52.115274Z","shell.execute_reply.started":"2024-06-29T07:33:48.807025Z","shell.execute_reply":"2024-06-29T07:33:52.114476Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"config = {\"max_epochs\":10,\n          \"val_check_interval\":0.2, # how many times we want to validate during an epoch\n          \"check_val_every_n_epoch\":1,\n          \"gradient_clip_val\":1.0,\n          \"num_training_samples_per_epoch\": 800,\n          \"lr\":3e-5,\n          \"train_batch_sizes\": [8],\n          \"val_batch_sizes\": [1],\n          # \"seed\":2022,\n          \"num_nodes\": 1,\n          \"warmup_steps\": 300, # 800/8*30/10, 10%\n          \"result_path\": \"./result\",\n          \"verbose\": False,\n          }","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:52.116298Z","iopub.execute_input":"2024-06-29T07:33:52.116595Z","iopub.status.idle":"2024-06-29T07:33:52.121856Z","shell.execute_reply.started":"2024-06-29T07:33:52.116563Z","shell.execute_reply":"2024-06-29T07:33:52.120976Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping , ModelCheckpoint , ModelPruning\n\nwandb_logger = WandbLogger(project=\"Donut_Knowledge_Distillation\", name=\"demo-run-cord-KD_500_test\")\n\nclass PushToHubCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n        pl_module.model.push_to_hub(\"Edgar404/donut-shivi-cheques_KD_500_test\",\n                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n\n    def on_train_end(self, trainer, pl_module):\n        print(f\"Pushing model to the hub after training\")\n        pl_module.processor.push_to_hub(\"Edgar404/donut-shivi-cheques_KD_500_test\",\n                                    commit_message=f\"Training done\")\n        pl_module.model.push_to_hub(\"Edgar404/donut-shivi-cheques-cheques_KD_500_test\",\n                                    commit_message=f\"Training done\")\n\nearly_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=10, verbose=False, mode=\"min\")\ncheckpoint = ModelCheckpoint(monitor =\"val_edit_distance\", dirpath = './best_model')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:52.123090Z","iopub.execute_input":"2024-06-29T07:33:52.123445Z","iopub.status.idle":"2024-06-29T07:33:52.216686Z","shell.execute_reply.started":"2024-06-29T07:33:52.123395Z","shell.execute_reply":"2024-06-29T07:33:52.215718Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from torch import nn","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:52.217979Z","iopub.execute_input":"2024-06-29T07:33:52.218811Z","iopub.status.idle":"2024-06-29T07:33:52.223389Z","shell.execute_reply.started":"2024-06-29T07:33:52.218774Z","shell.execute_reply":"2024-06-29T07:33:52.222476Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model_module = DonutModelPLModule(config, processor, model)\n\ntrainer = pl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        max_epochs=config.get(\"max_epochs\"),\n        val_check_interval=config.get(\"val_check_interval\"),\n        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n        gradient_clip_val=config.get(\"gradient_clip_val\"),\n        precision='32', # we'll use mixed precision\n        num_sanity_val_steps=0,\n        logger=wandb_logger,\n        callbacks=[PushToHubCallback(), early_stop_callback , checkpoint],)\n\ntrainer.fit(model_module)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T07:33:52.224619Z","iopub.execute_input":"2024-06-29T07:33:52.224930Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20240629_074352-y0n1v2z7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation/runs/y0n1v2z7' target=\"_blank\">demo-run-cord-KD_500_test</a></strong> to <a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation' target=\"_blank\">https://wandb.ai/eded_404/Donut_Knowledge_Distillation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eded_404/Donut_Knowledge_Distillation/runs/y0n1v2z7' target=\"_blank\">https://wandb.ai/eded_404/Donut_Knowledge_Distillation/runs/y0n1v2z7</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working/best_model exists and is not empty.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"407cfd5dd3a4431fb30dcdbb44d14693"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub, epoch 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807aefc453f346c5bb8e86d468ea4074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f400ecfc09642ee89287e0f84342272"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub, epoch 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b14e3b42154ec7813fbecd59aba0f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub, epoch 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"223c0a0de6484495b4bf6872c6c93f98"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Pushing model to the hub, epoch 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e610ad184685427eb0f310fe01196c07"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install jiwer\nimport jiwer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.push_to_hub('donut-shivi-cheques_KD_320',commit_message = 'best model updated')\n# processor.push_to_hub('donut-shivi-cheques_KD_320')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#teacher import\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"Edgar404/donut-shivi-cheques_KD_500_test\")\nprocessor = DonutProcessor.from_pretrained(\"Edgar404/donut-shivi-cheques_KD_500_test\")\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:41:10.652328Z","iopub.execute_input":"2024-06-29T10:41:10.652731Z","iopub.status.idle":"2024-06-29T10:41:35.273328Z","shell.execute_reply.started":"2024-06-29T10:41:10.652702Z","shell.execute_reply":"2024-06-29T10:41:35.272295Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee85b66f0ddb4cc9ac006f629a4c8554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25937e77178b4fc6b72f1434210277c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"702c4da997bc48d7ad66fd68ce0c1c3c"}},"metadata":{}},{"name":"stderr","text":"2024-06-29 10:41:21.964774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 10:41:21.964873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 10:41:22.083438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6f7f3d7f8647bc81ea8f979014e226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d060d5b9f74268a617b0c59f6e5012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be3a723c4b04cc48205fe2ef9a58423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5f9bfd6c72422c9311e81e395e7ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/382 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d881183af0440fb88d4f4815d2d29ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56542804c2684e92a1312faeb8114f18"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): DonutSwinModel(\n    (embeddings): DonutSwinEmbeddings(\n      (patch_embeddings): DonutSwinPatchEmbeddings(\n        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): DonutSwinEncoder(\n      (layers): ModuleList(\n        (0): DonutSwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x DonutSwinLayer(\n              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (attention): DonutSwinAttention(\n                (self): DonutSwinSelfAttention(\n                  (query): Linear(in_features=128, out_features=128, bias=True)\n                  (key): Linear(in_features=128, out_features=128, bias=True)\n                  (value): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): DonutSwinSelfOutput(\n                  (dense): Linear(in_features=128, out_features=128, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): DonutSwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n              (intermediate): DonutSwinIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): DonutSwinOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): DonutSwinPatchMerging(\n            (reduction): Linear(in_features=512, out_features=256, bias=False)\n            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): DonutSwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x DonutSwinLayer(\n              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (attention): DonutSwinAttention(\n                (self): DonutSwinSelfAttention(\n                  (query): Linear(in_features=256, out_features=256, bias=True)\n                  (key): Linear(in_features=256, out_features=256, bias=True)\n                  (value): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): DonutSwinSelfOutput(\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): DonutSwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n              (intermediate): DonutSwinIntermediate(\n                (dense): Linear(in_features=256, out_features=1024, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): DonutSwinOutput(\n                (dense): Linear(in_features=1024, out_features=256, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): DonutSwinPatchMerging(\n            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): DonutSwinStage(\n          (blocks): ModuleList(\n            (0-13): 14 x DonutSwinLayer(\n              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (attention): DonutSwinAttention(\n                (self): DonutSwinSelfAttention(\n                  (query): Linear(in_features=512, out_features=512, bias=True)\n                  (key): Linear(in_features=512, out_features=512, bias=True)\n                  (value): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): DonutSwinSelfOutput(\n                  (dense): Linear(in_features=512, out_features=512, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): DonutSwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n              (intermediate): DonutSwinIntermediate(\n                (dense): Linear(in_features=512, out_features=2048, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): DonutSwinOutput(\n                (dense): Linear(in_features=2048, out_features=512, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): DonutSwinPatchMerging(\n            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): DonutSwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x DonutSwinLayer(\n              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (attention): DonutSwinAttention(\n                (self): DonutSwinSelfAttention(\n                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): DonutSwinSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): DonutSwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (intermediate): DonutSwinIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): DonutSwinOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n    )\n    (pooler): AdaptiveAvgPool1d(output_size=1)\n  )\n  (decoder): MBartForCausalLM(\n    (model): MBartDecoderWrapper(\n      (decoder): MBartDecoder(\n        (embed_tokens): Embedding(57536, 1024, padding_idx=1)\n        (embed_positions): MBartLearnedPositionalEmbedding(1538, 1024)\n        (layers): ModuleList(\n          (0-3): 4 x MBartDecoderLayer(\n            (self_attn): MBartAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MBartAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (lm_head): Linear(in_features=1024, out_features=57536, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from copy import deepcopy","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:41:35.275237Z","iopub.execute_input":"2024-06-29T10:41:35.275885Z","iopub.status.idle":"2024-06-29T10:41:35.280072Z","shell.execute_reply.started":"2024-06-29T10:41:35.275849Z","shell.execute_reply":"2024-06-29T10:41:35.278976Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def modify_state_dict(state_dict):\n    \"\"\"\n    Modifies the state dictionary keys by removing the 'model.' prefix.\n    \n    Args:\n    state_dict (dict): Original state dictionary with keys having 'model.' prefix.\n    \n    Returns:\n    dict: Modified state dictionary with 'model.' prefix removed from keys.\n    \"\"\"\n    modified_state_dict = {}\n    \n    for key, value in state_dict.items():\n        # Remove the 'model.' prefix from the key\n        new_key = key.replace(\"model.\", \"\", 1)\n        modified_state_dict[new_key] = value\n    \n    return modified_state_dict","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:41:41.895171Z","iopub.execute_input":"2024-06-29T10:41:41.895858Z","iopub.status.idle":"2024-06-29T10:41:41.902282Z","shell.execute_reply.started":"2024-06-29T10:41:41.895825Z","shell.execute_reply":"2024-06-29T10:41:41.901129Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# import best model\ncheckpoint = torch.load('/kaggle/working/best_model/epoch=2-step=3920.ckpt')\nbest_model = deepcopy(model)\n\nmodified_state_dict = modify_state_dict(checkpoint['state_dict'])\nbest_model.load_state_dict(modified_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:42:29.931472Z","iopub.execute_input":"2024-06-29T10:42:29.932173Z","iopub.status.idle":"2024-06-29T10:42:32.375224Z","shell.execute_reply.started":"2024-06-29T10:42:29.932143Z","shell.execute_reply":"2024-06-29T10:42:32.374286Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Function to process an image and to return the OCR\ndef process_image(image , model , processor, d_type = torch.float32):\n    \"\"\" Function that takes an image and perform an OCR using the model DonUT via the task document\n    parsing\n    \n    parameters\n    __________\n    image : a machine readable image of class PIL or numpy\"\"\"\n    \n    task_prompt = \"<s_cord-v2>\"\n    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n    outputs = model.generate(\n        pixel_values.to(device, dtype = d_type),\n        decoder_input_ids=decoder_input_ids.to(device),\n        max_length=model.decoder.config.max_position_embeddings,\n        pad_token_id=processor.tokenizer.pad_token_id,\n        eos_token_id=processor.tokenizer.eos_token_id,\n        use_cache=True,\n        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n        return_dict_in_generate=True,\n    )\n\n    sequence = processor.batch_decode(outputs.sequences)[0]\n    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n    output = processor.token2json(sequence)\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:42:35.055753Z","iopub.execute_input":"2024-06-29T10:42:35.056464Z","iopub.status.idle":"2024-06-29T10:42:35.064750Z","shell.execute_reply.started":"2024-06-29T10:42:35.056427Z","shell.execute_reply":"2024-06-29T10:42:35.063836Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def weighted_cer(prediction_dict, reference_dict):\n    \"\"\"\n    Calcul du taux d'erreur de caractère (Character Error Rate - CER)\n    entre les valeurs d'un dictionnaire de référence  et les valeurs d'un dictionnaire prédit.\n    Chaque prediction est pondérée par la taille de la chaîne de caractère de référence\n    \"\"\"\n    \n    keys  = list(reference_dict.keys())\n    weights = []\n    cers = []\n    \n    for key in keys :\n        reference = str(reference_dict[key])\n        weight  = len(reference) \n\n        if key in prediction_dict.keys():\n            prediction = str(prediction_dict[key])\n            cer = jiwer.cer(reference,prediction)\n        else :\n            cer = 1\n            \n        weights.append(weight)\n        cers.append(cer)\n        \n    weights = np.array(weights)    \n    cers = np.array(cers)\n    \n    return (weights*cers).sum() / weights.sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:42:35.914789Z","iopub.execute_input":"2024-06-29T10:42:35.915156Z","iopub.status.idle":"2024-06-29T10:42:35.922398Z","shell.execute_reply.started":"2024-06-29T10:42:35.915127Z","shell.execute_reply":"2024-06-29T10:42:35.921465Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# util function to evaluate the model\n\ndef test_model_numeric(model , processor , sample_df , d_type = torch.float32 ):\n    \n    cer_list = []\n    time_list = []\n    failed_img_list = []\n\n    for idx in tqdm(range(len(sample_df))):\n            image = sample_df['image'][idx].convert('RGB')\n            real_dict = literal_eval(sample_df['ground_truth'][idx])\n\n            start_time = time.time()\n            output = process_image(image, model , processor , d_type)\n            end_time = time.time()\n\n            predicted_dict = output\n            inference_time = end_time - start_time\n\n            cer = weighted_cer(predicted_dict,real_dict)\n\n           \n            cer_list.append(cer)\n            time_list.append(inference_time)\n            \n    return  np.array(cer_list).mean() , np.array(time_list).mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:42:36.560015Z","iopub.execute_input":"2024-06-29T10:42:36.560489Z","iopub.status.idle":"2024-06-29T10:42:36.568236Z","shell.execute_reply.started":"2024-06-29T10:42:36.560452Z","shell.execute_reply":"2024-06-29T10:42:36.567026Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sample_df = processed_dataset['validation'].select(range(20))","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:42:37.577243Z","iopub.execute_input":"2024-06-29T10:42:37.577634Z","iopub.status.idle":"2024-06-29T10:42:37.584905Z","shell.execute_reply.started":"2024-06-29T10:42:37.577603Z","shell.execute_reply":"2024-06-29T10:42:37.583850Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import jiwer","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:43:29.857141Z","iopub.execute_input":"2024-06-29T10:43:29.857557Z","iopub.status.idle":"2024-06-29T10:43:29.862121Z","shell.execute_reply.started":"2024-06-29T10:43:29.857521Z","shell.execute_reply":"2024-06-29T10:43:29.861214Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# evaluation\ndevice = 'cuda'\n# model.to(device)\nmean_cer , mean_time =  test_model_numeric(model.to(device) , processor , sample_df , d_type = torch.float32 )","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:43:49.511756Z","iopub.execute_input":"2024-06-29T10:43:49.512422Z","iopub.status.idle":"2024-06-29T10:43:55.121214Z","shell.execute_reply.started":"2024-06-29T10:43:49.512391Z","shell.execute_reply":"2024-06-29T10:43:55.120336Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3366195c1074503a8c386c595b1a4b9"}},"metadata":{}}]},{"cell_type":"code","source":"mean_cer","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:43:55.846578Z","iopub.execute_input":"2024-06-29T10:43:55.847396Z","iopub.status.idle":"2024-06-29T10:43:55.853048Z","shell.execute_reply.started":"2024-06-29T10:43:55.847362Z","shell.execute_reply":"2024-06-29T10:43:55.852053Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0.028163018412060353"},"metadata":{}}]},{"cell_type":"code","source":"mean_time","metadata":{"execution":{"iopub.status.busy":"2024-06-29T10:43:58.342287Z","iopub.execute_input":"2024-06-29T10:43:58.342652Z","iopub.status.idle":"2024-06-29T10:43:58.348744Z","shell.execute_reply.started":"2024-06-29T10:43:58.342626Z","shell.execute_reply":"2024-06-29T10:43:58.347766Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.25507813692092896"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}